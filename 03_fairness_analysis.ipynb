{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a7280b-dce8-492c-b077-76b0486991ff",
   "metadata": {},
   "source": [
    "Fase 3 â€“ Fairness & Bias Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44322599-4c4a-4e27-a3a8-383ffb77c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Metrics by gender group:\n",
      "        accuracy   recall  selection_rate\n",
      "sex                                      \n",
      "Female  0.907850  0.20000        0.030717\n",
      "Male    0.788573  0.49602        0.208983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. Imports and data ---\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"data/raw/adult.csv\").dropna(subset=[\"sex\", \"race\", \"income\"])\n",
    "\n",
    "# Encode categorical columns\n",
    "le = LabelEncoder()\n",
    "for col in df.select_dtypes(include=\"object\"):\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop(\"income\", axis=1)\n",
    "y = df[\"income\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# --- 2. Train model ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(solver=\"saga\", max_iter=5000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# --- 3. Fairness analysis ---\n",
    "# Sensitive feature (gender)\n",
    "sensitive_features = df.loc[X_test.index, \"sex\"]\n",
    "\n",
    "# Create MetricFrame\n",
    "mf = MetricFrame(\n",
    "    metrics={\n",
    "        \"accuracy\": accuracy_score,\n",
    "        \"recall\": recall_score,\n",
    "        \"selection_rate\": selection_rate\n",
    "    },\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred,\n",
    "    sensitive_features=sensitive_features\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š Metrics by gender group:\")\n",
    "mf_df = mf.by_group.rename(index={0: \"Female\", 1: \"Male\"})\n",
    "print(mf_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7147528e-feae-4b5d-b201-9844edf8eac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex\n",
      "0    0.102389\n",
      "1    0.307058\n",
      "Name: ground_truth_positive_rate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Base rate (verdadeiro) por grupo\n",
    "gt_rate = (\n",
    "    pd.DataFrame({\n",
    "        \"sex\": df.loc[X_test.index, \"sex\"],\n",
    "        \"y_true\": y_test\n",
    "    })\n",
    "    .groupby(\"sex\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .rename(\"ground_truth_positive_rate\")\n",
    ")\n",
    "print(gt_rate)  # ex.: Female 0.06 vs Male 0.20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12ed29fe-709e-45a6-b553-a065af7fd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR/Recall  Female: 0.2\n",
      "TPR/Recall  Male  : 0.496\n",
      "FPR Female: 0.011\n",
      "FPR Male  : 0.082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "\n",
    "# Separar por grupo\n",
    "mask_f = (df.loc[X_test.index, \"sex\"] == 0)  # Female\n",
    "mask_m = (df.loc[X_test.index, \"sex\"] == 1)  # Male\n",
    "\n",
    "# Recall (TPR) por grupo â€“ entre as pessoas >50K, quantas o modelo acerta?\n",
    "tpr_f = recall_score(y_test[mask_f], y_pred[mask_f])  # equal opportunity para Female\n",
    "tpr_m = recall_score(y_test[mask_m], y_pred[mask_m])  # equal opportunity para Male\n",
    "print(\"TPR/Recall  Female:\", round(tpr_f, 3))\n",
    "print(\"TPR/Recall  Male  :\", round(tpr_m, 3))\n",
    "\n",
    "# FPR por grupo â€“ entre as pessoas â‰¤50K, quantas o modelo erra como >50K?\n",
    "def fpr(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return fp / (fp + tn)\n",
    "\n",
    "fpr_f = fpr(y_test[mask_f], y_pred[mask_f])\n",
    "fpr_m = fpr(y_test[mask_m], y_pred[mask_m])\n",
    "print(\"FPR Female:\", round(fpr_f, 3))\n",
    "print(\"FPR Male  :\", round(fpr_m, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2eb6a04b-0744-40fc-a3a2-af8022a12130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demographic Parity Difference: 0.178\n",
      "Equalized Odds Difference    : 0.296\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "sensitive = df.loc[X_test.index, \"sex\"]\n",
    "dp_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive)  # seleÃ§Ã£o\n",
    "eo_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive)     # TPR & FPR\n",
    "print(\"Demographic Parity Difference:\", round(dp_diff, 3))\n",
    "print(\"Equalized Odds Difference    :\", round(eo_diff, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486708d-16e7-4a9d-a42a-bade0a1b9791",
   "metadata": {},
   "source": [
    "# The initial logistic regression model exhibited a gender bias,\n",
    "# predicting significantly more high-income individuals among men than women.\n",
    "# This imbalance reflected historical patterns in the dataset, where male samples\n",
    "# were overrepresented in the >50K income category.\n",
    "#\n",
    "# To mitigate this bias, the Fairlearn ThresholdOptimizer was applied\n",
    "# with the 'true_positive_rate_parity' (Equal Opportunity) constraint.\n",
    "# This approach adjusted the decision thresholds separately for each gender group,\n",
    "# ensuring that women and men had an equal chance (recall rate) of being correctly\n",
    "# identified as high-income individuals.\n",
    "#\n",
    "# After applying the fairness mitigation, the recall rates between groups became\n",
    "# nearly equal (around 0.36 for both genders), while overall model performance\n",
    "# remained stable. This demonstrates a fairer and more responsible AI model\n",
    "# that balances predictive accuracy with ethical accountability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bb8806c-bb4d-448e-b760-4bbd9a2a29e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š After Fairness Mitigation:\n",
      "     accuracy    recall  selection_rate\n",
      "sex                                    \n",
      "0    0.888613  0.357576        0.082222\n",
      "1    0.777880  0.361194        0.136877\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.metrics import MetricFrame, selection_rate\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "\n",
    "# Sensitive feature\n",
    "sensitive = df.loc[X_test.index, \"sex\"]\n",
    "\n",
    "# PÃ³s-processamento usando o modelo diretamente\n",
    "postproc = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"true_positive_rate_parity\",  # Equal Opportunity\n",
    "    predict_method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "# Ajustar e prever\n",
    "postproc.fit(X_test_scaled, y_test, sensitive_features=sensitive)\n",
    "y_post = postproc.predict(X_test_scaled, sensitive_features=sensitive)\n",
    "\n",
    "# Avaliar novamente\n",
    "mf_post = MetricFrame(\n",
    "    metrics={\"accuracy\": accuracy_score, \"recall\": recall_score, \"selection_rate\": selection_rate},\n",
    "    y_true=y_test,\n",
    "    y_pred=y_post,\n",
    "    sensitive_features=sensitive\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š After Fairness Mitigation:\")\n",
    "print(mf_post.by_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4989016a-9d23-4ffe-9ca9-6c4ca00645ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
