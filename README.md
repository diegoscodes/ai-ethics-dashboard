---
# ğŸ§  AI Monitoring & Ethics Dashboard

A professional **end-to-end Responsible AI project** that demonstrates how to **monitor, evaluate, and mitigate bias** in machine learning models using **Fairlearn** and **SHAP**, and visualize results through an interactive **Streamlit dashboard**.

---

## ğŸ“‹ Project Overview

This project shows how organizations can **apply Responsible AI principles** by detecting and mitigating algorithmic bias.
It uses the **Adult Income Dataset** to predict whether an individual earns more than **$50K/year**, while ensuring fairness across sensitive attributes such as **gender** and **race**.

---

## âš™ï¸ Tech Stack

* ğŸ **Python 3.11**
* ğŸ¤– **Scikit-learn** â€“ model training (Logistic Regression)
* âš–ï¸ **Fairlearn** â€“ fairness evaluation & mitigation
* ğŸ§© **SHAP** â€“ model explainability (global & local)
* ğŸŒ **Streamlit** â€“ interactive dashboard
* ğŸ“Š **Plotly** & **Matplotlib** â€“ data visualization
* ğŸ“¦ **Pandas / NumPy / Joblib / TQDM** â€“ data processing utilities

---

## ğŸ§© Project Structure

```
ai-ethics-dashboard/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ dashboard.py          # Streamlit app (Fairness + SHAP)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ adult.csv         # Dataset (Adult Income)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â””â”€â”€ 03_fairness_analysis.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run

1. **Clone this repository:**

   ```bash
   git clone https://github.com/diegoscodes/ai-ethics-dashboard.git
   cd ai-ethics-dashboard
   ```

2. **Create a virtual environment:**

   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate     # (Windows)
   source .venv/bin/activate    # (macOS/Linux)
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

4. **Launch the dashboard:**

   ```bash
   streamlit run app/dashboard.py
   ```

---

## ğŸ“Š Key Features

âœ… **Fairness Analysis (Before vs After)**
â€ƒCompare accuracy, recall, and selection rate across sensitive groups.

âœ… **Bias Mitigation (Equal Opportunity)**
â€ƒApplies Fairlearnâ€™s ThresholdOptimizer to balance true positive rates.

âœ… **Explainability (SHAP)**
â€ƒVisualizes global feature importance and local prediction insights.

âœ… **Sensitive Attribute Selection**
â€ƒSwitch between *gender* and *race* to analyze fairness from different perspectives.

---

## ğŸ” Results Summary

| Attribute        | Before Mitigation         | After Mitigation         | Observation                   |
| ---------------- | ------------------------- | ------------------------ | ----------------------------- |
| **Gender (sex)** | Recall (M: 0.50, F: 0.20) | Recall (M/F â‰ˆ 0.36)      | Balanced recall achieved      |
| **Race**         | Moderate bias gap         | Reduced after mitigation | Fairer classification balance |

> After mitigation, the model achieved **~0.81 accuracy** with significantly reduced bias, proving fairness can coexist with good performance.

---

## ğŸ§  Learnings

* Bias often mirrors **real-world inequality** present in data.
* Responsible AI focuses on **understanding, not hiding**, sensitive variables.
* **Fairness â‰  perfection** â€” itâ€™s an **ongoing monitoring process**.

---

## ğŸ“· Preview

*Add a screenshot or GIF of your Streamlit dashboard here.*
Example:
![Dashboard Preview](app/assets/dashboard_preview.png)

---

## ğŸ‘¤ Author

**Diego Ferreira**
ğŸŒ [LinkedIn](https://www.linkedin.com/in/diegoscodes) â€¢ ğŸ’» [GitHub](https://github.com/diegoscodes)

---

ğŸ§© *Built as part of an AI & Machine Learning professional portfolio project demonstrating ethical, explainable, and fair model development.*
