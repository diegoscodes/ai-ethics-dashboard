ğŸ§  AI Monitoring & Ethics Dashboard

A professional end-to-end Responsible AI project that demonstrates how to monitor, evaluate, and mitigate bias in machine learning models using Fairlearn and SHAP, and visualize results through an interactive Streamlit dashboard.

ğŸ“‹ Project Overview

This project shows how organizations can apply Responsible AI principles by detecting and mitigating algorithmic bias.
It uses the Adult Income Dataset to predict whether an individual earns more than $50K/year, while ensuring fairness across sensitive attributes such as gender and race.

âš™ï¸ Tech Stack

ğŸ Python 3.11

ğŸ¤– Scikit-learn â€“ model training (Logistic Regression + Random Forest)

âš–ï¸ Fairlearn â€“ fairness evaluation & mitigation

ğŸ§© SHAP â€“ model explainability (global & local)

ğŸŒ Streamlit â€“ interactive dashboard

ğŸ“Š Plotly & Matplotlib â€“ data visualization

ğŸ“¦ Pandas / NumPy / Joblib / TQDM â€“ data processing utilities

ğŸ“ ReportLab â€“ PDF report export (Conclusions tab)

ğŸ§© Project Structure
ai-ethics-dashboard/
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ dashboard.py              # Streamlit app (Performance, Fairness, Comparison, Group Fairness, Conclusions)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚       â””â”€â”€ adult.csv             # Dataset (Adult Income)
â”‚
â”œâ”€â”€ models/                       # Saved artifacts (after training notebooks)
â”‚   â”œâ”€â”€ logistic_regression.joblib
â”‚   â”œâ”€â”€ random_forest.joblib
â”‚   â”œâ”€â”€ scaler.joblib
â”‚   â””â”€â”€ encoders_label.joblib
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb   # GridSearchCV / cross-validation & model export
â”‚   â””â”€â”€ 03_fairness_analysis.ipynb
â”‚
â”œâ”€â”€ reports/
â”‚	â””â”€â”€ final_report.pdf          # Exported from the dashboard (Conclusions tab)
â”‚
â”‚
â”œâ”€â”€ assets/                       # ğŸ“¸ Static media for README and dashboard
â”‚   â”œâ”€â”€ performance_tab.png
â”‚   â”œâ”€â”€ fairness_tab.png
â”‚   â”œâ”€â”€ comparison_tab.png
â”‚   â”œâ”€â”€ group_fairness_tab.png
â”‚   â””â”€â”€ conclusions_tab.png
â”‚
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md



ğŸš€ Key Features

âœ… Multiple models: Logistic Regression (baseline) and Random Forest (tuned via GridSearchCV)
âœ… Performance monitoring: Accuracy, F1-score, ROC-AUC
âœ… Fairness monitoring: Demographic Parity Difference, Equalized Odds Difference
âœ… Group Fairness: per-group metrics and plots for sex and race (selection rate, F1)
âœ… Global Comparison Mode: a single toggle compares models across all tabs (Performance, Fairness, Group Fairness)
âœ… Conclusions tab: executive summary with ethical interpretation and recommendations
âœ… PDF Export: generate a professional report directly from the dashboard (ReportLab)

Mitigation options (e.g., ThresholdOptimizer/Reweighing) are discussed and prototyped in notebooks and planned for interactive integration.

â–¶ï¸ How to Run

Clone

git clone https://github.com/diegoscodes/ai-ethics-dashboard.git
cd ai-ethics-dashboard


Virtual env

python -m venv .venv
.\.venv\Scripts\activate     # Windows
# source .venv/bin/activate  # macOS/Linux


Install

pip install -r requirements.txt


Launch

streamlit run app/dashboard.py

ğŸ§­ Using the Dashboard

Model selector (sidebar): choose baseline (LogReg) or tuned (Random Forest).

Compare both models side by side (sidebar): enables the Global Comparison Mode, updating all tabs to show both models (metrics, tables and grouped charts).

Conclusions tab: view an executive summary and click â€œExport Detailed Report as PDFâ€ to generate /reports/final_report.pdf.

ğŸ“Š Results (current run)

Random Forest (best tuned params via GridSearchCV)

Accuracy â‰ˆ 0.858

F1-score â‰ˆ 0.676

ROC-AUC â‰ˆ 0.911

EO diff â‰ˆ 0.079 (lower is fairer)

Logistic Regression (baseline)

Accuracy â‰ˆ 0.825

F1-score â‰ˆ 0.557

ROC-AUC â‰ˆ 0.854

EO diff â‰ˆ 0.261

Takeaway: Random Forest achieves a stronger performanceâ€“fairness balance and is the recommended candidate for deployment, with ongoing ethical monitoring.

ğŸ” Group Fairness (Sex & Race)

Per-group selection rate and F1 reveal how predictions differ across sex and race.

Smaller gaps between bars indicate more equitable behavior.

The dashboard highlights potential disparities with automatic insights (success/info/warning).

ğŸ§  Learnings

Dataset bias often reflects real-world inequality; monitoring + mitigation are essential.

Fairness is not static â€” it requires continuous auditing (concept/data drift).

Balancing interpretability vs. performance is a design decision; using a transparent baseline plus a tuned ensemble provides both.

ğŸ“ PDF Reporting

Inside ğŸ§¾ Conclusions, click â€œExport Detailed Report as PDFâ€ to generate:

Results summary (both models),

Interpretation & ethical insights,

Recommendations,

Final reflection,

Results Summary & Challenges (explicitly addressing the assignmentâ€™s â€œitem 8â€).

The file is saved to: reports/final_report.pdf.
G
ğŸ“· Preview (optional)


assets/
â”œâ”€â”€ performance_tab.png
â”œâ”€â”€ fairness_tab.png
â”œâ”€â”€ comparison_tab.png
â”œâ”€â”€ group_fairness_tab.png
â””â”€â”€ conclusions_tab.png



![Dashboard â€“ Comparison](assets/performance_tab.png)

test

## ğŸ“· Dashboard Previews

| Performance | Fairness | Comparison |
|--------------|-----------|-------------|
| ![Performance](assets/performance_tab.png) | ![Fairness](assets/fairness_tab.png) | ![Comparison](assets/comparison_tab.png) |

| Group Fairness | Conclusions |
|----------------|-------------|
| ![Group Fairness](assets/group_fairness_tab.png) | ![Conclusions](assets/conclusions_tab.png) |

ğŸ‘¤ Author

Diego Ferreira
ğŸŒ Ireland â€¢ ğŸ”— LinkedIn
 â€¢ ğŸ’» GitHub

Built as part of a professional Responsible AI portfolio project combining performance, fairness, explainability, and reporting.